{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81ad0120",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6aea5c",
   "metadata": {},
   "source": [
    "Python version spiral ... \n",
    "- > tf 2.11.0 introduced changes that break recommenders integration, need to step back in time \n",
    "- pyenv install 3.9\n",
    "- pyenv virtualenv 3.9 recommenders\n",
    "- pyenv uninstall recommenders\n",
    "- pyenv activate recommenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83962e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env TF_USE_LEGACY_KERAS=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914076d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1862e356",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_parquet(\"../data/processed/reviews_small.parquet\")\n",
    "items = pd.read_parquet(\"../data/processed/items_small.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8255337",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a83408",
   "metadata": {},
   "outputs": [],
   "source": [
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94627dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_users(reviews): \n",
    "    \"\"\"\n",
    "    Given reviews, generate a user dataframe     \n",
    "    \"\"\"\n",
    "    users = reviews.groupby(['user_id']).rating.count()\n",
    "    users = pd.DataFrame(users).reset_index()\n",
    "    users.rename(columns={'rating':'ratings'}, inplace=True)\n",
    "    return users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa2ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = extract_users(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042fac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items = set(items.item_id)\n",
    "reviews = reviews[reviews.item_id.isin(all_items)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00344bd",
   "metadata": {},
   "source": [
    "â—in the notebook, ratings are thresholded ... do we need to follow suit? what are the ramifications if we don't? OH... in the notebook, a click is an interaction, there's no middle ground. the network is going to operate on 0s or 1s. by leaving low reviews in our matrix, the network would learn to recommend things users have interacted with, but not necessarily positively. our case is the same, a review is an interaction. we're aiming to recommend, and we should not want to recommend low reviews. so filter... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c389eeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discard reviews by users outside our core group \n",
    "reviews = reviews[reviews.user_id.isin(set(users.user_id))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015fcc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0a5f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df79e2df",
   "metadata": {},
   "source": [
    "### Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e447f6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a055980",
   "metadata": {},
   "source": [
    "### Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a885026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we need sparse types, use the scipy COO since it seems to be incorporated in both pytorch and recommenders\n",
    "from scipy import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78279fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.datasets.sparse import AffinityMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3727ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.drop(['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1572e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {\n",
    "    \"col_user\": \"user_id\",\n",
    "    \"col_item\": \"item_id\",\n",
    "    \"col_rating\": \"rating\",\n",
    "}\n",
    "ui_sparse = AffinityMatrix(reviews, **header)\n",
    "\n",
    "# This isn't implied by the name, but this densifies the matrix, i.e. we have a contiguous u x i\n",
    "# matrix here (user vector of item ratings) ... though it's actually not clear how the memory is \n",
    "# managed underneath in scipy, the 'dense' array might just be a bunch of pointers to the DFs stored \n",
    "# in the AM object... \n",
    "ui_dense, u_map, i_map = ui_sparse.gen_affinity_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26538469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "print(sys.getsizeof(ui_sparse))\n",
    "print(sys.getsizeof(ui_sparse.df))\n",
    "print(sys.getsizeof(ui_dense))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6594c080",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef8aadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(u_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4cf5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ui_dense[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7acd5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ui_dense[1][[0,413]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ea4d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.nonzero(ui_dense[0])[0]\n",
    "b = np.nonzero(ui_dense[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f95cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([a, b], axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b9d73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ui_dense[0][np.nonzero(ui_dense[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db02bf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80be98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_a = cosine_similarity([ui_dense[0]], [ui_dense[33]])\n",
    "sims = [0] *len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca166cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims[0] = sim_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ec4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4b12b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[0]*2]*top_k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36ddb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db591d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr \n",
    "\n",
    "def pearson_similarity(a, b):\n",
    "    \"\"\"\n",
    "    Compute Pearson similarity\n",
    "    \"\"\"\n",
    "    return (1 + pearsonr(a, b).statistic) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0662c077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populating a full user similarity matrix is inherently limited by (and is a questionable \n",
    "# strategy because of) the u^2 memory requirement. Unlike our affinity matrices, \n",
    "# these are not sparse. Since our goal is recommending items, we'll compute the similarity\n",
    "# iterativealy and store the most similar users to get down to C * u memory\n",
    "similarity_matrix = np.array([[0.] * top_k] * len(users))\n",
    "for a in range(len(users)): \n",
    "\n",
    "    # Collect our similarities w/ respect to user A\n",
    "    sim_a = {}\n",
    "    for b in range(len(users)): \n",
    "        if a != b: \n",
    "            \n",
    "            # Given the sparsity of our review vectors, cosine similarity is going to be \n",
    "            # effectively zero if we look across the entire item space... compare only those \n",
    "            # items these two users have in common (at least 1 rating between the two). \n",
    "            a_item_ix = np.nonzero(ui_dense[a])[0]            \n",
    "            b_item_ix = np.nonzero(ui_dense[b])[0]\n",
    "            all_ix = np.concatenate([a_item_ix, b_item_ix])\n",
    "            a_items = ui_dense[a][all_ix]            \n",
    "            b_items = ui_dense[b][all_ix]\n",
    "\n",
    "            # Fill non-ratings with middling scores. Non-interactions appear \n",
    "            # dissimilar to positive reviews and similar to negative ones otherwise.\n",
    "            a_items[a_items==0] = 4\n",
    "            b_items[b_items==0] = 2\n",
    "            \n",
    "            # Cosine similarity risks insensitivity to rating value, while imperfect here, \n",
    "            # Pearson similarity gets us sensitivty to rating magnitude and trends\n",
    "            sim_a[b] = pearson_similarity(a_items, b_items)\n",
    "        \n",
    "    # Find and store the top k user matches, in order    \n",
    "    # NOTE: dict sorting logic courtesy of gpt-4o (https://chatgpt.com/share/687dc72f-54b4-8013-806e-b1de20d0ef12)\n",
    "    top = sorted(sim_a.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    similarity_matrix[a] = [x[0] for x in top]\n",
    "\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0e22a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a_items[-7] = 2\n",
    "a_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2eefd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e73b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity([a_items],[b_items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5ebe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_similarity(a_items, b_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accf5e2b",
   "metadata": {},
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bbe07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.datasets.sparse import AffinityMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf9522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only relevant to VAE strategy \n",
    "reviews = reviews[reviews.rating >= 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acff7ffe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e12fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608aec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a454f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Strategy adapted from tutorials available in the Recommenders project, see \n",
    "# https://github.com/recommenders-team/recommenders/tree/main\n",
    "from recommenders.datasets.python_splitters import python_random_split\n",
    "\n",
    "# Split along user boundaries to ensure no leakage of preference between train and test\n",
    "train_users, test_users, val_users = python_random_split(users, [.9, .05, .05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630664c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_users.shape, test_users.shape, val_users.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4d4532",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = reviews[reviews.user_id.isin(train_users.user_id)]\n",
    "val = reviews[reviews.user_id.isin(val_users.user_id)]\n",
    "test = reviews[reviews.user_id.isin(test_users.user_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc5ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape, val.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc06db88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technique from Recommenders (see https://github.com/recommenders-team/recommenders/blob/45e1b215a35e69b92390e16eb818d4528d0a33a2/examples/02_model_collaborative_filtering/standard_vae_deep_dive.ipynb) \n",
    "# to improve utility of validation set during training - only allow items in\n",
    "# the validation set that are also present in the train set\n",
    "val = val[val.item_id.isin(train.item_id.unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbcf82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.datasets.python_splitters import python_stratified_split \n",
    "\n",
    "# Another technique employed in Recommenders (see above link for notebook), for in-flight validation to be \n",
    "# meaningful during training, our validation set needs not just ground truth, but unseen validation samples \n",
    "# to see if predictions for validation users are relevant (to those users). Anyway, break down our val and test \n",
    "# sets again to support this strategy\n",
    "val_src, val_target = python_stratified_split(\n",
    "    data=val, \n",
    "    ratio=0.8, \n",
    "    filter_by=\"item\", \n",
    "    col_user=\"user_id\", \n",
    "    col_item=\"item_id\"\n",
    "    )\n",
    "test_src, test_target = python_stratified_split(\n",
    "    data=test, \n",
    "    ratio=0.8, \n",
    "    filter_by=\"item\", \n",
    "    col_user=\"user_id\", \n",
    "    col_item=\"item_id\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b6946",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val.shape, \" -> \", val_src.shape, val_target.shape)\n",
    "print(test.shape, \" -> \", test_src.shape, test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5b4f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to use standard names across the analysis \n",
    "header = {\n",
    "        \"col_user\": \"user_id\",\n",
    "        \"col_item\": \"item_id\",\n",
    "        \"col_rating\": \"rating\",\n",
    "        # Unclear why this doesn't also eat a timestamp, but many of the functions that split temporally use, fortunately \n",
    "        # the column 'timestamp' (i.e. DEFAULT_TIMESTAMP_COL='timestamp') so I think we're fine. \n",
    "        # \"col_timestamp\" : \"timestamp\"\n",
    "    }\n",
    "\n",
    "train_matrix = AffinityMatrix(df=train, **header)\n",
    "val_matrix = AffinityMatrix(df=val, **header)\n",
    "val_src_matrix = AffinityMatrix(df=val_src, **header)\n",
    "val_tgt_matrix = AffinityMatrix(df=val_target, **header)\n",
    "test_src_matrix = AffinityMatrix(df=test_src, **header)\n",
    "test_tgt_matrix = AffinityMatrix(df=test_target, **header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6664d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This generates a sparse array of user vectors, aka user-item matrix\n",
    "# X[0] is the first user in the list, with entries for all items known when the matrix was constructed in that row\n",
    "train, _, _ = train_matrix.gen_affinity_matrix()\n",
    "val, _, _ = val_matrix.gen_affinity_matrix() \n",
    "val_src, _, _ = val_src_matrix.gen_affinity_matrix()\n",
    "val_tgt, _, _ = val_tgt_matrix.gen_affinity_matrix()\n",
    "test_src, _, _ = test_src_matrix.gen_affinity_matrix()\n",
    "test_tgt, _, _ = test_src_matrix.gen_affinity_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ed4afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.utils.python_utils import binarize\n",
    "\n",
    "train = binarize(train, 3)\n",
    "val = binarize(train, 3)\n",
    "val_src = binarize(val_src, 3) \n",
    "val_tgt = binarize(val_tgt, 3)\n",
    "test_src = binarize(test_src, 3)\n",
    "test_tgt = binarize(test_tgt, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d68d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make sure this is reported during training/configuration\n",
    "sparsity = np.count_nonzero(train)/(train.shape[0]*train.shape[1])*100\n",
    "print(f\"sparsity: {sparsity:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585601b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.__version__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985102a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.models.vae.standard_vae import StandardVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2bf5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StandardVAE(\n",
    "    n_users = train.shape[0], \n",
    "    original_dim = train.shape[1],\n",
    "    intermediate_dim=250, \n",
    "    latent_dim=50, \n",
    "    n_epochs=1, \n",
    "    batch_size=1, \n",
    "    k=10, \n",
    "    verbose=1, \n",
    "    seed=4, \n",
    "    save_path=\"models/svae.hdf5\", \n",
    "    drop_encoder=0.5, \n",
    "    drop_decoder=0.5, \n",
    "    annealing=False, \n",
    "    beta=1.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56eb549",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cc01bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x_train=train, \n",
    "    x_valid=val, \n",
    "    x_val_tr=val_src, \n",
    "    x_val_te=val_tgt, \n",
    "    mapper=val_matrix,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307d4cbf",
   "metadata": {},
   "source": [
    "Managing text-based reviews at this scale could be a challenge, and I'd like to steer clear of LLMs for this effort. We could do an embedding on the review and use that for similarity, but we have pretty rich item data. Perhaps let's ignore the collaborative aspect here and build a shopping interface that: \n",
    "- surfaces the most popular items, and encourages you to add items to your shopping cart for a big discount/promo\n",
    "- based on clicks and cart items, improves the recommendations and surfaces new products\n",
    "\n",
    "We can use an autoencoder to accept a sparse matrix of users and items, learn to reproduce that matrix, and in so doing support prediction on missing values. However, this matrix is of size users x items, which here is 1.8e7 x 1.6e6 = 28,125,000,000 KB (best-case, higher if stored as np floats) ~= 26 TB !! WTF. \n",
    "- In the standard VAE example (https://github.com/recommenders-team/recommenders/blob/main/examples/02_model_collaborative_filtering/standard_vae_deep_dive.ipynb) the clicks are turned into a histogram for each user ... so we have n_user vector of length n_items... then I guess each of these is a training sample. The VAE presumably learns, given a sparse user vector, to predict every rating. This takes the complexity down and gives us a training set we can iterate over. \n",
    "\n",
    "Let's avoid any distributional pressure (present in VAE, SVAE, disentangled VAE) and go for a basic autoencoder using the strategy laid out above, i.e."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891606e6",
   "metadata": {},
   "source": [
    "### Scratch Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cbbcb4",
   "metadata": {},
   "source": [
    "That damned Recommenders VAE is a dependency dumpster fire ... walking away after 10h of fighting crusty environments that generate more errors than outcomes. Shift to a basic autoencoder in pytorch and just eat the cost of having to implement our own validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59907dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd37f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0501d7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e6a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math \n",
    "import torch \n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from sparse import AffinityMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba6e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoencoder\n",
    "\n",
    "    NOTE: with cues from https://www.geeksforgeeks.org/deep-learning/implementing-an-autoencoder-in-pytorch/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dims=1000):\n",
    "        \"\"\"\n",
    "        Initialize a new object given an item count \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(dims, 500),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(500, 75),\n",
    "            nn.ReLU(), \n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(75, 500),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(500, dims),\n",
    "            nn.ReLU(), \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Implement our forward pass \n",
    "        \"\"\"\n",
    "        h = self.encoder(x) \n",
    "        r = self.decoder(h)\n",
    "\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d97ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCartDataset(torch.utils.data.Dataset): \n",
    "    \"\"\"\n",
    "    Custom pytorch-compatible dataset. Adapted from \n",
    "    https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files\n",
    "    \"\"\"\n",
    "    def __init__(self, users, reviews): \n",
    "        \"\"\"\n",
    "        Initialize a new instance\n",
    "\n",
    "        Oof. The ideal pattern here is for the dataset to be blissfully ignorant of our split strategy and just \n",
    "        make a dataset available to its client based on the raw data passed. However the split strategy is rather intricate below ... \n",
    "        note the five splits. Can we easily raise that up to a higher level? The refactoring might not be trivial and it may result in \n",
    "        residue of this split strategy bleeding over to the other models -- what's common and what's not? \n",
    "\n",
    "        common: \n",
    "        - train, val, test split\n",
    "        - val test and test test bonus splits - for all validation stages we need to check performance, however non-NN techniques \n",
    "          really only need a test split, right? if we do train holdout for validation, we essentially have three test sets (train-test, val, test)\n",
    "        - a need to operate on the same validation or at least test data, lest the comparison be biased by the selection method each model applies\n",
    "\n",
    "        unique\n",
    "        - logic to prune reviews < 3.5 -- we don't do this in cfnn, and naive doesn't care (predicts highest review in the matrix), if this is \n",
    "          done during training, it will also need to be done during inference\n",
    "        - need for a pytorch-style dataset ... the naive method is doing a O(n) search, the cfnn needs dataframes -- while refactoring is \n",
    "          possible, why understake the risk it will be a disjoint and inelegant fit? \n",
    "        - the VAE implementation wants all train and val, but doesn't require a test dataset. we ou\n",
    "\n",
    "\n",
    "        we could: \n",
    "        - pass train and val, hold test out\n",
    "        - pass test to predict function, which we need for the demo anyway\n",
    "        - keep the pytorch dataset unique to the pytorch-compatible class... doesn't make sense to try and foist on other algos... we are \n",
    "        doing this in the wrong order, filtering and then splitting... we need to outsource the splitting and then do the filtering inside each \n",
    "        model \n",
    "\n",
    "        right now this is speculation, just get something working! we can figure out how to streamline after -- oh, but we need a dataset \n",
    "        implementation\n",
    "        \"\"\"\n",
    "        self.users = users \n",
    "        self.reviews = reviews \n",
    "        self.matrix = \n",
    "\n",
    "    def build_affinity_matrices(): \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        oof\n",
    "\n",
    "    def split(users, reviews, items):\n",
    "        \"\"\"\n",
    "        Generate splits \n",
    "        \"\"\"\n",
    "        print(f\"Full user-item matrix is {len(users) * len(items)}\")\n",
    "\n",
    "        # We are trying to teach the model what a good interaction is like, and we'll \n",
    "        # ultimately be interested only in whether to recommend an item or not ... \n",
    "        # low reviews are not something we want the model suggesting... \n",
    "        reviews_low = reviews[reviews.rating < 3]\n",
    "        reviews = reviews[reviews.rating >= 3]\n",
    "\n",
    "        # NOTE: Strategy adapted from tutorials available in the Recommenders project, see \n",
    "        # https://github.com/recommenders-team/recommenders/tree/main\n",
    "        # Split along user boundaries to ensure no leakage of preference between train and test\n",
    "        train_users, test_users, val_users = python_random_split(users, [.9, .05, .05])\n",
    "        print(train_users.shape, test_users.shape, val_users.shape)\n",
    "\n",
    "        train = reviews[reviews.user_id.isin(train_users.user_id)]\n",
    "        val = reviews[reviews.user_id.isin(val_users.user_id)]\n",
    "        test = reviews[reviews.user_id.isin(test_users.user_id)]\n",
    "        print(train.shape, val.shape, test.shape)\n",
    "        \n",
    "        # Technique from Recommenders (see https://github.com/recommenders-team/recommenders/blob/45e1b215a35e69b92390e16eb818d4528d0a33a2/examples/02_model_collaborative_filtering/standard_vae_deep_dive.ipynb) \n",
    "        # to improve utility of validation set during training - only allow items in\n",
    "        # the validation set that are also present in the train set\n",
    "        val = val[val.item_id.isin(train.item_id.unique())]\n",
    "        print(val.shape)\n",
    "\n",
    "        # Another technique employed in Recommenders (see above link for notebook), for in-flight validation to be \n",
    "        # meaningful during training, our validation set needs not just ground truth, but unseen validation samples \n",
    "        # to see if predictions for validation users are relevant (to those users). Anyway, break down our val and test \n",
    "        # sets again to support this strategy\n",
    "        val_src, val_target = python_stratified_split(\n",
    "            data=val, \n",
    "            ratio=0.8, \n",
    "            filter_by=\"item\", \n",
    "            col_user=\"user_id\", \n",
    "            col_item=\"item_id\"\n",
    "            )\n",
    "        test_src, test_target = python_stratified_split(\n",
    "            data=test, \n",
    "            ratio=0.8, \n",
    "            filter_by=\"item\", \n",
    "            col_user=\"user_id\", \n",
    "            col_item=\"item_id\"\n",
    "            )\n",
    "        \n",
    "        print(val.shape, \" -> \", val_src.shape, val_target.shape)\n",
    "        print(test.shape, \" -> \", test_src.shape, test_target.shape)\n",
    "\n",
    "        train_matrix = AffinityMatrix(df=train, **header)\n",
    "        val_matrix = AffinityMatrix(df=val, **header)\n",
    "        val_src_matrix = AffinityMatrix(df=val_src, **header)\n",
    "        val_tgt_matrix = AffinityMatrix(df=val_target, **header)\n",
    "        test_src_matrix = AffinityMatrix(df=test_src, **header)\n",
    "        test_tgt_matrix = AffinityMatrix(df=test_target, **header)\n",
    "\n",
    "        # This generates a sparse array of user vectors, aka user-item matrix\n",
    "        # X[0] is the first user in the list, with entries for all items known when the matrix was constructed in that row\n",
    "        train, _, _ = train_matrix.gen_affinity_matrix()\n",
    "        val, _, _ = val_matrix.gen_affinity_matrix() \n",
    "        val_src, _, _ = val_src_matrix.gen_affinity_matrix()\n",
    "        val_tgt, _, _ = val_tgt_matrix.gen_affinity_matrix()\n",
    "        test_src, _, _ = test_src_matrix.gen_affinity_matrix()\n",
    "        test_tgt, _, _ = test_src_matrix.gen_affinity_matrix()    \n",
    "\n",
    "        train = binarize(train, 3)\n",
    "        val = binarize(train, 3)\n",
    "        val_src = binarize(val_src, 3) \n",
    "        val_tgt = binarize(val_tgt, 3)\n",
    "        test_src = binarize(test_src, 3)\n",
    "        test_tgt = binarize(test_tgt, 3)\n",
    "\n",
    "        sparsity = np.count_nonzero(train)/(train.shape[0]*train.shape[1])*100\n",
    "        print(f\"sparsity: {sparsity:.2f}%\")\n",
    "    def __len__(self): \n",
    "        \"\"\"\n",
    "        Retrieve length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.img_labels) \n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        \"\"\"\n",
    "        Retrieve an item at the provided index\n",
    "        \"\"\"\n",
    "        #TODO: implement\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d893b2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(batch_size=5, shuffle=True): \n",
    "    \"\"\"\n",
    "    Retrieve a pytorch-style dataloader \n",
    "    \"\"\"\n",
    "\n",
    "    #TODO: implement\n",
    "    #transform = transforms.Compose([\n",
    "    #     transforms.ConvertImageDtype(torch.float),\n",
    "    #     transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    #])\n",
    "\n",
    "    #data = DeepCartDataset(transform=transform)\n",
    "    #loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    #return loader\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032a91f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader, model, loss_interval=20, epochs=2, lr=0.01, momentum=0.9):\n",
    "    \"\"\"\n",
    "    Train the model with the provided dataset\n",
    "\n",
    "    NOTE: this is a similar training loop as we used for our vision model in the \n",
    "    the vision project, forward pass\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    train_loss = []\n",
    "\n",
    "    tqdm.write(f\"Starting training run...\")    \n",
    "    # TODO: configure WandB\n",
    "    # see https://docs.wandb.ai/guides/integrations/pytorch/\n",
    "    config = {}\n",
    "    run = wandb.init(config=config) \n",
    "\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(loader):\n",
    "\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # collect metrics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (i % loss_interval) == (loss_interval - 1): \n",
    "                train_loss.append(running_loss / loss_interval)\n",
    "                tqdm.write(f\"[{epoch + 1}, {i + 1:5d}] loss: {running_loss / loss_interval:.3f}\")\n",
    "                running_loss = 0 \n",
    "    \n",
    "    tqdm.write(\"Training complete!\") \n",
    "\n",
    "    return train_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7f6037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
