{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81ad0120",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83962e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env TF_USE_LEGACY_KERAS=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914076d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tf_keras as keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1862e356",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_parquet(\"../data/2023/Electronics_min.parquet\")\n",
    "items = pd.read_parquet(\"../data/2023/meta_Electronics.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8255337",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a83408",
   "metadata": {},
   "outputs": [],
   "source": [
    "items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49be17af",
   "metadata": {},
   "source": [
    "We need to build a compact user-centric representation of preference, collapse review data into a sparse matrix of user -> item preferences. There are some heuristics that need to be applied in the process: \n",
    "1. users with few interactions are a very weak signal -- without associations with multiple products, we are not teaching the model about positive associations\n",
    "2. products with few interactions are also a very weak signal -- we are looking to connect users and items that have tiny interaction graphs are not going to improve our macro-level predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00609d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This should be a configuration option hyper parameter, we can relax this if training isn't suepr computationally expensive\n",
    "min_ratings = 1000\n",
    "items = items[items.rating_number > min_ratings]\n",
    "len(items) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042fac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above item filtering serves to reduce the computational complexity as well as \n",
    "# reduce sparsity, before we filter reviews make sure we remove those associated with \n",
    "# dropped items\n",
    "all_items = set(items.parent_asin)\n",
    "reviews = reviews[reviews.parent_asin.isin(all_items)]\n",
    "reviews.rename(columns={'parent_asin':'item_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af200300",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = reviews.groupby(['user_id']).rating.count()\n",
    "users = pd.DataFrame(users).reset_index()\n",
    "users.rename(columns={'rating':'ratings'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9b879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a configuration parameter, as above ... relax if we don't have issues with compute\n",
    "min_reviews = 10\n",
    "users = users[users.ratings > min_reviews] \n",
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00344bd",
   "metadata": {},
   "source": [
    "â—in the notebook, ratings are thresholded ... do we need to follow suit? what are the ramifications if we don't? OH... in the notebook, a click is an interaction, there's no middle ground. the network is going to operate on 0s or 1s. by leaving low reviews in our matrix, the network would learn to recommend things users have interacted with, but not necessarily positively. our case is the same, a review is an interaction. we're aiming to recommend, and we should not want to recommend low reviews. so filter... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e76856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: decide if we need to keep the low reviews around \n",
    "reviews = reviews[reviews.rating >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c389eeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discard reviews by users outside our core group \n",
    "reviews = reviews[reviews.user_id.isin(set(users.user_id))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015fcc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94423619",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_size = len(users) * len(items)\n",
    "matrix_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bbe07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.datasets.sparse import AffinityMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0a5f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e12fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608aec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a454f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Strategy adapted from tutorials available in the Recommenders project, see \n",
    "# https://github.com/recommenders-team/recommenders/tree/main\n",
    "from recommenders.datasets.python_splitters import python_random_split\n",
    "\n",
    "# Split along user boundaries to ensure no leakage of preference between train and test\n",
    "train_users, test_users, val_users = python_random_split(users, [.9, .05, .05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630664c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_users.shape, test_users.shape, val_users.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4d4532",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = reviews[reviews.user_id.isin(train_users.user_id)]\n",
    "val = reviews[reviews.user_id.isin(val_users.user_id)]\n",
    "test = reviews[reviews.user_id.isin(test_users.user_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc5ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape, val.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc06db88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technique from Recommenders (see https://github.com/recommenders-team/recommenders/blob/45e1b215a35e69b92390e16eb818d4528d0a33a2/examples/02_model_collaborative_filtering/standard_vae_deep_dive.ipynb) \n",
    "# to improve utility of validation set during training - only allow items in\n",
    "# the validation set that are also present in the train set\n",
    "val = val[val.item_id.isin(train.item_id.unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbcf82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.datasets.python_splitters import python_stratified_split \n",
    "\n",
    "# Another technique employed in Recommenders (see above link for notebook), for in-flight validation to be \n",
    "# meaningful during training, our validation set needs not just ground truth, but unseen validation samples \n",
    "# to see if predictions for validation users are relevant (to those users). Anyway, break down our val and test \n",
    "# sets again to support this strategy\n",
    "val_src, val_target = python_stratified_split(\n",
    "    data=val, \n",
    "    ratio=0.8, \n",
    "    filter_by=\"item\", \n",
    "    col_user=\"user_id\", \n",
    "    col_item=\"item_id\"\n",
    "    )\n",
    "test_src, test_target = python_stratified_split(\n",
    "    data=test, \n",
    "    ratio=0.8, \n",
    "    filter_by=\"item\", \n",
    "    col_user=\"user_id\", \n",
    "    col_item=\"item_id\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b6946",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val.shape, \" -> \", val_src.shape, val_target.shape)\n",
    "print(test.shape, \" -> \", test_src.shape, test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5b4f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to use standard names across the analysis \n",
    "header = {\n",
    "        \"col_user\": \"user_id\",\n",
    "        \"col_item\": \"item_id\",\n",
    "        \"col_rating\": \"rating\",\n",
    "        # Unclear why this doesn't also eat a timestamp, but many of the functions that split temporally use, fortunately \n",
    "        # the column 'timestamp' (i.e. DEFAULT_TIMESTAMP_COL='timestamp') so I think we're fine. \n",
    "        # \"col_timestamp\" : \"timestamp\"\n",
    "    }\n",
    "\n",
    "train_matrix = AffinityMatrix(df=train, **header)\n",
    "val_src_matrix = AffinityMatrix(df=val_src, **header)\n",
    "val_tgt_matrix = AffinityMatrix(df=val_target, **header)\n",
    "test_src_matrix = AffinityMatrix(df=test_src, **header)\n",
    "test_tgt_matrix = AffinityMatrix(df=test_target, **header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6664d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This generates a sparse array of user vectors, aka user-item matrix\n",
    "# X[0] is the first user in the list, with entries for all items known when the matrix was constructed in that row\n",
    "train, _, _ = train_matrix.gen_affinity_matrix()\n",
    "val_src, _, _ = val_src_matrix.gen_affinity_matrix()\n",
    "val_tgt, _, _ = val_tgt_matrix.gen_affinity_matrix()\n",
    "test_src, _, _ = test_src_matrix.gen_affinity_matrix()\n",
    "test_tgt, _, _ = test_src_matrix.gen_affinity_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ed4afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.utils.python_utils import binarize\n",
    "\n",
    "train = binarize(train, 3)\n",
    "val_src = binarize(val_src, 3) \n",
    "val_tgt = binarize(val_tgt, 3)\n",
    "test_src = binarize(test_src, 3)\n",
    "test_tgt = binarize(test_tgt, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d68d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make sure this is reported during training/configuration\n",
    "sparsity = np.count_nonzero(train)/(train.shape[0]*train.shape[1])\n",
    "sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aaa640",
   "metadata": {},
   "source": [
    "## Model Design "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585601b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.__version__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985102a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.models.vae.standard_vae import StandardVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2bf5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StandardVAE(\n",
    "    n_users = train.shape[0], \n",
    "    original_dim = train.shape[1],\n",
    "    intermediate_dim=250, \n",
    "    latent_dim=50, \n",
    "    n_epochs=1, \n",
    "    batch_size=1, \n",
    "    k=10, \n",
    "    verbose=1, \n",
    "    seed=4, \n",
    "    save_path=\"models/svae.hdf5\", \n",
    "    drop_encoder=0.5, \n",
    "    drop_decoder=0.5, \n",
    "    annealing=False, \n",
    "    beta=1.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cc01bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train=train, \n",
    "          x_valid=val, \n",
    "          x_val_tr=val_src, \n",
    "          x_val_te=val_tgt, \n",
    "          mapper=AffinityMatrix(val),\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307d4cbf",
   "metadata": {},
   "source": [
    "Managing text-based reviews at this scale could be a challenge, and I'd like to steer clear of LLMs for this effort. We could do an embedding on the review and use that for similarity, but we have pretty rich item data. Perhaps let's ignore the collaborative aspect here and build a shopping interface that: \n",
    "- surfaces the most popular items, and encourages you to add items to your shopping cart for a big discount/promo\n",
    "- based on clicks and cart items, improves the recommendations and surfaces new products\n",
    "\n",
    "We can use an autoencoder to accept a sparse matrix of users and items, learn to reproduce that matrix, and in so doing support prediction on missing values. However, this matrix is of size users x items, which here is 1.8e7 x 1.6e6 = 28,125,000,000 KB (best-case, higher if stored as np floats) ~= 26 TB !! WTF. \n",
    "- In the standard VAE example (https://github.com/recommenders-team/recommenders/blob/main/examples/02_model_collaborative_filtering/standard_vae_deep_dive.ipynb) the clicks are turned into a histogram for each user ... so we have n_user vector of length n_items... then I guess each of these is a training sample. The VAE presumably learns, given a sparse user vector, to predict every rating. This takes the complexity down and gives us a training set we can iterate over. \n",
    "\n",
    "Let's avoid any distributional pressure (present in VAE, SVAE, disentangled VAE) and go for a basic autoencoder using the strategy laid out above, i.e."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891606e6",
   "metadata": {},
   "source": [
    "## Autoencoder Prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e6a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import pandas as pd \n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import math \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba6e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoencoder\n",
    "\n",
    "    NOTE: with cues from https://www.geeksforgeeks.org/deep-learning/implementing-an-autoencoder-in-pytorch/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dims=1000):\n",
    "        \"\"\"\n",
    "        Initialize a new object \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(dims, 500),\n",
    "            nn.Linear(500, 75),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(75, 500),\n",
    "            nn.Linear(500, dims),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Implement our forward pass \n",
    "        \"\"\"\n",
    "        h = self.encoder(x) \n",
    "        r = self.decoder(h)\n",
    "\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d97ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCartDataset(torch.utils.data.Dataset): \n",
    "    \"\"\"\n",
    "    Custom pytorch-compatible dataset. Adapted from \n",
    "    https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files\n",
    "    \"\"\"\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None): \n",
    "\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "\n",
    "        #TODO: implement\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.img_labels) \n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        #TODO: implement\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d893b2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(batch_size=5, shuffle=True): \n",
    "    \"\"\"\n",
    "    Retrieve a pytorch-style dataloader \n",
    "    \"\"\"\n",
    "\n",
    "    #TODO: implement\n",
    "    #transform = transforms.Compose([\n",
    "    #     transforms.ConvertImageDtype(torch.float),\n",
    "    #     transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    #])\n",
    "\n",
    "    #data = DeepCartDataset(transform=transform)\n",
    "    #loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    #return loader\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032a91f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader, model, loss_interval=20, epochs=2, lr=0.01, momentum=0.9):\n",
    "    \"\"\"\n",
    "    Train the model with the provided dataset\n",
    "\n",
    "    NOTE: this is a similar training loop as we used for our vision model in the \n",
    "    the vision project, forward pass\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    train_loss = []\n",
    "\n",
    "    tqdm.write(f\"Starting training run...\")    \n",
    "    # TODO: configure WandB\n",
    "    # see https://docs.wandb.ai/guides/integrations/pytorch/\n",
    "    config = {}\n",
    "    run = wandb.init(config=config) \n",
    "\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(loader):\n",
    "\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # collect metrics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (i % loss_interval) == (loss_interval - 1): \n",
    "                train_loss.append(running_loss / loss_interval)\n",
    "                tqdm.write(f\"[{epoch + 1}, {i + 1:5d}] loss: {running_loss / loss_interval:.3f}\")\n",
    "                running_loss = 0 \n",
    "    \n",
    "    tqdm.write(\"Training complete!\") \n",
    "\n",
    "    return train_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7f6037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
